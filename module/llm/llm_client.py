# module/llm/llm_client.py
from openai import OpenAI
import re
import json
import os
import threading
from datetime import datetime

from module.utils.utils import load_history, load_system_prompt_cached, load_emotion_prompt, load_dialogue_prompt, logger
from module.params import OPENAI_MODEL, OPENAI_TEMPERATURE, OPENAI_TOP_P, OPENAI_MAX_TOKENS
from module.mongo.emotion_dataset import get_recent_dialogue_history
from module.emotion.basic_personality import get_top_long_emotions
from module.emotion.emotion_stats import load_current_emotion


client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


def generate_gpt_response_from_history() -> str:
    """
    MongoDBã‹ã‚‰ç›´è¿‘3ä»¶ã®å¯¾è©±å±¥æ­´ã¨ç¾åœ¨æ„Ÿæƒ…ã‚’å–å¾—ã—ã€ãã‚Œã‚’ã‚‚ã¨ã«GPTå¿œç­”ã‚’ç”Ÿæˆã€‚
    å¿œç­”å†…å®¹ã¯æ–‡å­—åˆ—ã§è¿”å´ï¼ˆJSONæŠ½å‡ºã¯åˆ¥ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§å‡¦ç†ï¼‰ã€‚
    """
    logger.info("[START] generate_gpt_response_from_history")

    system_prompt = load_system_prompt_cached()
    user_prompt = load_dialogue_prompt()

    # å±¥æ­´å–å¾—ï¼ˆç›´è¿‘3ä»¶ï¼‰
    logger.info("[INFO] å±¥æ­´å–å¾—ä¸­...")
    selected_history = load_history(3)
    logger.info(f"[INFO] å±¥æ­´ä»¶æ•°: {len(selected_history)} ä»¶")

    # ç¾åœ¨æ„Ÿæƒ…ã®å–å¾—
    from module.emotion.emotion_stats import load_current_emotion
    current_emotion = load_current_emotion()
    logger.info(f"[INFO] ç¾åœ¨æ„Ÿæƒ…ãƒ™ã‚¯ãƒˆãƒ«: {current_emotion}")

    # æ„Ÿæƒ…ãƒ™ã‚¯ãƒˆãƒ«ã‚’AIã®å†…çš„æ„Ÿæƒ…ã¨ã—ã¦æ˜ç¤º
    if current_emotion:
        emotion_text = (
            "\nã€ç¾åœ¨ã®æ„Ÿæƒ…çŠ¶æ…‹ï¼ˆAIè‡ªèº«ã®å†…çš„çŠ¶æ…‹ï¼‰ã€‘\n"
            "ã‚ãªãŸï¼ˆAIï¼‰ã¯ä»¥ä¸‹ã®æ„Ÿæƒ…ã‚’ç¾åœ¨æŠ±ã„ã¦ã„ã¾ã™ã€‚\n"
            "ã“ã®æ„Ÿæƒ…ã«å¾“ã£ã¦ã€è¨€è‘‰é£ã„ã‚„æ…‹åº¦ã€èªå°¾ãªã©ã‚’è‡ªç„¶ã«èª¿æ•´ã—ã¦ãã ã•ã„ã€‚\n"
            + ", ".join([f"{k}: {v}%" for k, v in current_emotion.items()])
        )
    else:
        emotion_text = (
            "\nã€ç¾åœ¨ã®æ„Ÿæƒ…çŠ¶æ…‹ï¼ˆAIè‡ªèº«ã®å†…çš„çŠ¶æ…‹ï¼‰ã€‘\n"
            "ç¾åœ¨ã®æ„Ÿæƒ…ã¯ã¾ã ååˆ†ã«è“„ç©ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚é€šå¸¸ã®å£èª¿ã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚"
        )

    # GPTã¸ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰
    try:
        logger.info("[INFO] OpenAIå‘¼ã³å‡ºã—é–‹å§‹")
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                *[{"role": entry["role"], "content": entry["message"]} for entry in selected_history],
                {"role": "user", "content": f"{emotion_text}\n\n{user_prompt}"}
            ],
            max_tokens=OPENAI_MAX_TOKENS,
            temperature=OPENAI_TEMPERATURE,
            top_p=OPENAI_TOP_P
        )
        logger.info("[INFO] OpenAIå¿œç­”å–å¾—å®Œäº†")
        return response.choices[0].message.content.strip()

    except Exception as e:
        logger.error(f"[ERROR] OpenAIå‘¼ã³å‡ºã—å¤±æ•—: {e}")
        return "å¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"


def generate_emotion_from_prompt_with_context(
    user_input: str,
    emotion_structure: dict,
    best_match: dict | None
) -> tuple[str, dict]:
    """
    æ„Ÿæƒ…æ§‹é€ ã¨å‚ç…§æ„Ÿæƒ…ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦ã€GPTã«å¿œç­”ç”Ÿæˆã‚’è¡Œã‚ã›ã‚‹ã€‚
    best_match ãŒ None ã®å ´åˆã¯å±¥æ­´ãƒ™ãƒ¼ã‚¹ã§å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    """
    system_prompt = load_system_prompt_cached()
    user_prompt = load_dialogue_prompt()

    # ğŸ”¸ äººæ ¼å‚¾å‘ã®å–å¾—ã¨æ•´å½¢ï¼ˆlongã‚«ãƒ†ã‚´ãƒªã®é »å‡ºemotionï¼‰
    top4_personality = get_top_long_emotions()
    personality_text = "\nã€äººæ ¼å‚¾å‘ã€‘\nã“ã®AIã¯ä»¥ä¸‹ã®æ„Ÿæƒ…ã‚’æŒã¤å‚¾å‘ãŒã‚ã‚Šã¾ã™ï¼š\n"
    if top4_personality:
        for emotion, count in top4_personality:
            personality_text += f"ãƒ»{emotion}ï¼ˆ{count}å›ï¼‰\n"
    else:
        personality_text += "å‚¾å‘æƒ…å ±ãŒã¾ã ååˆ†ã«ã‚ã‚Šã¾ã›ã‚“ã€‚\n"

    # ğŸ”» æ¡ä»¶1ï¼šãƒãƒƒãƒãªã— â†’ å±¥æ­´ãƒ™ãƒ¼ã‚¹ã§ç”Ÿæˆ
    if best_match is None:
        fallback_response = generate_gpt_response_from_history()

        prompt = (
            f"{user_prompt}\n\n"
            f"{personality_text}\n"
            f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€: {user_input}\n"
            f"å±¥æ­´å¿œç­”: {fallback_response}\n\n"
            f"ã€æŒ‡ç¤ºã€‘ä¸Šè¨˜ã®äººæ ¼å‚¾å‘ã¨å±¥æ­´ã‚’å‚è€ƒã«ã€è‡ªç„¶ãªå¿œç­”ã®ã¿ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n"
            f"æ§‹æˆæ¯”ã‚„æ„Ÿæƒ…æ§‹é€ ã¯ä¸è¦ã§ã™ã€‚"
        )
        try:
            response = client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=OPENAI_MAX_TOKENS,
                temperature=OPENAI_TEMPERATURE,
                top_p=OPENAI_TOP_P
            )
            return response.choices[0].message.content.strip(), {}
        except Exception as e:
            logger.error(f"[ERROR] å¿œç­”ç”Ÿæˆå¤±æ•—: {e}")
            return "å¿œç­”ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", {}

    # ğŸ”» æ¡ä»¶2ï¼šãƒãƒƒãƒã‚ã‚Š â†’ æ„Ÿæƒ…ãƒ‡ãƒ¼ã‚¿å‚ç…§ã—ã¦å¿œç­”æ§‹ç¯‰
    from module.response.main_response import collect_all_category_responses

    emotion_name = best_match.get("emotion")
    date_str = best_match.get("date")
    history_data = collect_all_category_responses(emotion_name, date_str)

    reference_emotions = []
    for category in ["short", "intermediate", "long"]:
        data = history_data.get(category)
        if data:
            reference_emotions.append(data)

    reference_text = "\n\nã€AIè‡ªèº«ã®è¨˜æ†¶ï¼ˆå‚è€ƒæ„Ÿæƒ…ãƒ‡ãƒ¼ã‚¿ï¼‰ã€‘\n"
    reference_text += (
        "ä»¥ä¸‹ã¯ã€ã‚ãªãŸè‡ªèº«ï¼ˆAIï¼‰ãŒã“ã‚Œã¾ã§ã«çµŒé¨“ãƒ»è¨˜éŒ²ã—ãŸæ„Ÿæƒ…ã®è¨˜æ†¶ã§ã™ã€‚\n"
        "å„ã‚±ãƒ¼ã‚¹ã¯å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®å¯¾è©±ã«åŸºã¥ã„ã¦è¨˜éŒ²ã•ã‚ŒãŸã‚‚ã®ã§ã‚ã‚Šã€ã‚ãªãŸã®å†…çš„ãªè¨˜æ†¶ã¨ã—ã¦ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™ã€‚\n"
        "ã“ã‚Œã‚‰ã®è¨˜æ†¶ã‚’å‚ç…§ã—ãªãŒã‚‰ã€è‡ªç„¶ã§ä¸€è²«æ€§ã®ã‚ã‚‹å¿œç­”ã‚’æ§‹æˆã—ã¦ãã ã•ã„ã€‚\n"
    )

    for i, item in enumerate(reference_emotions, 1):
        reference_text += f"\nâ— è¨˜æ†¶ã‚±ãƒ¼ã‚¹{i}\n"
        reference_text += f"ä¸»æ„Ÿæƒ…: {item.get('ä¸»æ„Ÿæƒ…')}\n"
        reference_text += f"æ§‹æˆæ¯”: {item.get('æ§‹æˆæ¯”')}\n"
        reference_text += f"çŠ¶æ³: {item.get('çŠ¶æ³')}\n"
        reference_text += f"å¿ƒç†åå¿œ: {item.get('å¿ƒç†åå¿œ')}\n"
        reference_text += f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(item.get('keywords', []))}\n"

    prompt = (
        f"{user_prompt}\n\n"
        f"{personality_text}\n"
        f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€: {user_input}\n"
        f"{reference_text}\n\n"
        f"ã€æŒ‡ç¤ºã€‘ä¸Šè¨˜ã®æ„Ÿæƒ…å‚ç…§ãƒ‡ãƒ¼ã‚¿ã¨äººæ ¼å‚¾å‘ã‚’å‚è€ƒã«ã€emotion_promptã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦å¿œç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
        f"è‡ªç„¶ãªå¿œç­” + æ§‹æˆæ¯” + JSONå½¢å¼ã®æ„Ÿæƒ…æ§‹é€ ã®é †ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
    )

    try:
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            max_tokens=OPENAI_MAX_TOKENS,
            temperature=OPENAI_TEMPERATURE,
            top_p=OPENAI_TOP_P
        )
        full_response = response.choices[0].message.content.strip()
        json_match = re.search(r"```json\s*(\{.*?\})\s*```", full_response, re.DOTALL)
        if json_match:
            try:
                emotion_data = json.loads(json_match.group(1))
                emotion_data["date"] = datetime.now().strftime("%Y%m%d%H%M%S")
                clean_response = re.sub(r"```json\s*\{.*?\}\s*```", "", full_response, flags=re.DOTALL).strip()

                # ğŸ”¸ éåŒæœŸã‚¹ãƒ¬ãƒƒãƒ‰ã§æ„Ÿæƒ…çµ±åˆå‡¦ç†ã‚’å®Ÿè¡Œ
                if "æ§‹æˆæ¯”" in emotion_data:
                    threading.Thread(
                        target=run_emotion_update_pipeline,
                        args=(emotion_data["æ§‹æˆæ¯”"],)
                    ).start()

                return clean_response, emotion_data
            except Exception as e:
                logger.error(f"[ERROR] JSONãƒ‘ãƒ¼ã‚¹å¤±æ•—: {e}")
                return full_response, {}
        else:
            return full_response, {}

    except Exception as e:
        logger.error(f"[ERROR] å¿œç­”ç”Ÿæˆå¤±æ•—: {e}")
        return "å¿œç­”ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", {}


# ğŸ”» éåŒæœŸã‚¹ãƒ¬ãƒƒãƒ‰ã§æ„Ÿæƒ…ãƒ™ã‚¯ãƒˆãƒ«åˆæˆãƒ»ä¿å­˜ãƒ»ã‚µãƒãƒªãƒ¼ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°
async def run_emotion_update_pipeline(new_vector: dict):
    try:
        from module.emotion.emotion_stats import (
            load_current_emotion,
            merge_emotion_vectors,
            save_current_emotion,
            summarize_feeling
        )

        current = load_current_emotion()
        merged = merge_emotion_vectors(current, new_vector)
        save_current_emotion(merged)
        summarize_feeling(merged)

    except Exception as e:
        logger.error(f"[ERROR] æ„Ÿæƒ…æ›´æ–°å‡¦ç†ã«å¤±æ•—: {e}")
