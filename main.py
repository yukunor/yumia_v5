import sys
import os
import re
import traceback

from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from pydantic import BaseModel
from fastapi.responses import FileResponse, JSONResponse

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ‘ã‚¹è¿½åŠ 
sys.path.append(os.path.join(os.path.dirname(__file__), "module"))

from utils import append_history, load_history, logger
from module.response.main_response import run_response_pipeline
import module.memory.main_memory as memory
from llm_client import extract_emotion_summary
from module.memory.index_emotion import extract_personality_tendency
from module.file_handler.file_router import handle_uploaded_file

app = FastAPI()

class UserMessage(BaseModel):
    message: str

def sanitize_output_for_display(text: str) -> str:
    text = re.sub(r"```json\s*\{.*?\}\s*```", "", text, flags=re.DOTALL)
    text = re.sub(r"\{\s*\"date\"\s*:\s*\".*?\".*?\"keywords\"\s*:\s*\[.*?\]\s*\}", "", text, flags=re.DOTALL)
    return text.strip()

@app.get("/")
def get_ui():
    return FileResponse("static/index.html")

@app.get("/history")
def get_history():
    try:
        return {"history": load_history()}
    except Exception as e:
        logger.exception("å±¥æ­´å–å¾—ä¸­ã«ä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        raise HTTPException(status_code=500, detail="å±¥æ­´ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")

@app.post("/chat")
async def chat(
    message: str = Form(...),
    file: UploadFile = File(None)
):
    logger.debug("âœ… /chat ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«åˆ°é”")
    try:
        user_input = message
        logger.debug(f"ğŸ“¥ ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å–å¾—å®Œäº†: {user_input}")

        if file:
            logger.debug(f"ğŸ“ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«å: {file.filename}")
            extracted_text = await handle_uploaded_file(file)  # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å‘¼ã³å‡ºã—
            if extracted_text:
                user_input += f"\n\n[æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹]:\n{extracted_text}"

        append_history("user", user_input)
        logger.debug("ğŸ“ ãƒ¦ãƒ¼ã‚¶ãƒ¼å±¥æ­´è¿½åŠ å®Œäº†")

        logger.debug("ğŸ” å¿œç­”ç”Ÿæˆã¨æ„Ÿæƒ…æ¨å®š é–‹å§‹")
        response, emotion_data = run_response_pipeline(user_input)
        logger.debug("âœ… å¿œç­”ã¨æ„Ÿæƒ…ãƒ‡ãƒ¼ã‚¿å–å¾— å®Œäº†")

        logger.info(f"ğŸ§¾ å–å¾—ã—ãŸæ„Ÿæƒ…ãƒ‡ãƒ¼ã‚¿ã®å†…å®¹: {emotion_data}")
        # LLMã«ã‚ˆã‚‹å¿œç­”ç”Ÿæˆã¨æ„Ÿæƒ…æ§‹é€ æŠ½å‡º
        # å®Ÿä½“ã¯ module/llm/llm_client.py ã® generate_emotion_from_prompt_with_context() ã‚’å‘¼ã³å‡ºã™
        summary = extract_emotion_summary(emotion_data, emotion_data.get("ä¸»æ„Ÿæƒ…", "æœªå®šç¾©"))
        logger.info(f"ğŸ“Š æ§‹æˆæ¯”ã‚µãƒãƒª: {summary}")

        logger.debug("ğŸ’¬ æœ€çµ‚å¿œç­”å†…å®¹ï¼ˆãã®ã¾ã¾è¡¨ç¤ºï¼‰:")
        logger.debug(f"ğŸ’­{response}")
        cleaned = summary.replace(f"ï¼ˆä¸»æ„Ÿæƒ…: {emotion_data.get('ä¸»æ„Ÿæƒ…')}ï½œæ§‹æˆæ¯”: ", "").rstrip("ï¼‰")
        logger.debug(f"ğŸ’æ§‹æˆæ¯”ï¼ˆä¸»æ„Ÿæƒ…: {emotion_data.get('ä¸»æ„Ÿæƒ…')}ï¼‰: {cleaned}")

        append_history("system", response)
        logger.debug("ğŸ“ å¿œç­”å±¥æ­´è¿½åŠ å®Œäº†")

        logger.debug("ğŸ’¾ æ„Ÿæƒ…ä¿å­˜å‡¦ç†ï¼ˆåŒæœŸå®Ÿè¡Œï¼‰é–‹å§‹")
        memory.handle_emotion(emotion_data)

        logger.debug("ğŸ§  äººæ ¼å‚¾å‘ã®æŠ½å‡º é–‹å§‹")
        tendency = extract_personality_tendency()
        logger.debug(f"ğŸ§­ ç¾åœ¨äººæ ¼å‚¾å‘: {tendency}")

        logger.debug("ğŸ“¤ å¿œç­”ã¨å±¥æ­´ã‚’è¿”å´")
        return JSONResponse(content={
            "message": response,
            "history": load_history(),
            "personality_tendency": tendency
        })

    except Exception as e:
        logger.error(f"[ERROR] /chat ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§ä¾‹å¤–ç™ºç”Ÿ: {e}")
        return JSONResponse(status_code=500, content={"error": str(e)})
